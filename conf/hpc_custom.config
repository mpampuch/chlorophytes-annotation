/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Custom HPC Configuration Template
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Template configuration for customizing SLURM settings based on your HPC system.
    Copy this file and modify the settings according to your HPC specifications.
----------------------------------------------------------------------------------------
*/

params {
    config_profile_name        = 'Custom HPC profile'
    config_profile_description = 'Custom configuration for your specific HPC system'
    
    // HPC-specific parameters (modify according to your system)
    hpc_account               = null          // SLURM account/project (e.g., 'myproject')
    hpc_partition_short       = 'short'       // Short jobs partition
    hpc_partition_normal      = 'normal'      // Normal jobs partition  
    hpc_partition_long        = 'long'        // Long jobs partition
    hpc_partition_highmem     = 'highmem'     // High memory partition
    hpc_partition_gpu         = 'gpu'         // GPU partition (if available)
    hpc_qos                   = null          // Quality of Service (if used)
    hpc_modules               = null          // Modules to load (comma-separated)
    
    // Storage paths (modify according to your system)
    scratch_dir               = "/scratch/\$USER"
    tmp_dir                   = "/tmp"
    singularity_cache_dir     = "/scratch/\$USER/singularity"
}

// Configure Singularity for your HPC environment
singularity {
    enabled      = true
    autoMounts   = true
    cacheDir     = params.singularity_cache_dir
    runOptions   = '--cleanenv --containall'
    
    // Add bind paths for your HPC system (uncomment and modify as needed)
    // runOptions = '--cleanenv --containall --bind /scratch --bind /project --bind /data'
}

process {
    executor = 'slurm'
    
    // Default SLURM settings
    queue = params.hpc_partition_normal
    
    // Add account if specified
    clusterOptions = { 
        def options = '--mail-type=FAIL'
        if (params.hpc_account) options += " --account=${params.hpc_account}"
        if (params.hpc_qos) options += " --qos=${params.hpc_qos}"
        return options
    }
    
    // Load modules if specified
    beforeScript = { 
        def script = ''
        if (params.hpc_modules) {
            params.hpc_modules.split(',').each { module ->
                script += "module load ${module.trim()}\n"
            }
        }
        return script
    }
    
    // Error handling optimized for HPC
    errorStrategy = { 
        if (task.exitStatus in [104, 134, 137, 139, 143, 247]) {
            return 'retry'
        } else if (task.exitStatus in [130, 131]) {
            return 'ignore'  // User cancelled
        } else {
            return 'finish'
        }
    }
    maxRetries = 3
    maxErrors  = '-1'
    
    // Resource labels with HPC-optimized settings
    withLabel:process_single {
        cpus   = 1
        memory = '4.GB'
        time   = '2.h'
        queue  = params.hpc_partition_short
    }
    
    withLabel:process_low {
        cpus   = { check_max( 2 * task.attempt, 'cpus' ) }
        memory = { check_max( 8.GB * task.attempt, 'memory' ) }
        time   = { check_max( 4.h * task.attempt, 'time' ) }
        queue  = params.hpc_partition_normal
    }
    
    withLabel:process_medium {
        cpus   = { check_max( 8 * task.attempt, 'cpus' ) }
        memory = { check_max( 32.GB * task.attempt, 'memory' ) }
        time   = { check_max( 8.h * task.attempt, 'time' ) }
        queue  = params.hpc_partition_normal
    }
    
    withLabel:process_high {
        cpus   = { check_max( 16 * task.attempt, 'cpus' ) }
        memory = { check_max( 64.GB * task.attempt, 'memory' ) }
        time   = { check_max( 16.h * task.attempt, 'time' ) }
        queue  = params.hpc_partition_normal
        clusterOptions = { 
            def options = '--mail-type=FAIL --exclusive'
            if (params.hpc_account) options += " --account=${params.hpc_account}"
            if (params.hpc_qos) options += " --qos=${params.hpc_qos}"
            return options
        }
    }
    
    withLabel:process_long {
        time  = { check_max( 48.h * task.attempt, 'time' ) }
        queue = params.hpc_partition_long
    }
    
    withLabel:process_high_memory {
        memory = { check_max( 256.GB * task.attempt, 'memory' ) }
        time   = { check_max( 24.h * task.attempt, 'time' ) }
        queue  = params.hpc_partition_highmem
        clusterOptions = { 
            def options = '--mail-type=FAIL --exclusive'
            if (params.hpc_account) options += " --account=${params.hpc_account}"
            if (params.hpc_qos) options += " --qos=${params.hpc_qos}"
            return options
        }
    }
    
    // Process-specific HPC configurations
    withName:EDTA {
        cpus   = { check_max( 24 * task.attempt, 'cpus' ) }
        memory = { check_max( 200.GB * task.attempt, 'memory' ) }
        time   = { check_max( 72.h * task.attempt, 'time' ) }
        queue  = params.hpc_partition_highmem
        clusterOptions = { 
            def options = '--mail-type=FAIL --exclusive'
            if (params.hpc_account) options += " --account=${params.hpc_account}"
            if (params.hpc_qos) options += " --qos=${params.hpc_qos}"
            return options
        }
    }
    
    withName:TRINITY {
        cpus   = { check_max( 32 * task.attempt, 'cpus' ) }
        memory = { check_max( 256.GB * task.attempt, 'memory' ) }
        time   = { check_max( 48.h * task.attempt, 'time' ) }
        queue  = params.hpc_partition_highmem
        clusterOptions = { 
            def options = '--mail-type=FAIL --exclusive'
            if (params.hpc_account) options += " --account=${params.hpc_account}"
            if (params.hpc_qos) options += " --qos=${params.hpc_qos}"
            return options
        }
    }
    
    withName:MAKER2 {
        cpus   = { check_max( 24 * task.attempt, 'cpus' ) }
        memory = { check_max( 200.GB * task.attempt, 'memory' ) }
        time   = { check_max( 72.h * task.attempt, 'time' ) }
        queue  = params.hpc_partition_highmem
        clusterOptions = { 
            def options = '--mail-type=FAIL --exclusive'
            if (params.hpc_account) options += " --account=${params.hpc_account}"
            if (params.hpc_qos) options += " --qos=${params.hpc_qos}"
            return options
        }
    }
    
    withName:BRAKER2 {
        cpus   = { check_max( 16 * task.attempt, 'cpus' ) }
        memory = { check_max( 128.GB * task.attempt, 'memory' ) }
        time   = { check_max( 48.h * task.attempt, 'time' ) }
        queue  = params.hpc_partition_highmem
        clusterOptions = { 
            def options = '--mail-type=FAIL --exclusive'
            if (params.hpc_account) options += " --account=${params.hpc_account}"
            if (params.hpc_qos) options += " --qos=${params.hpc_qos}"
            return options
        }
    }
    
    withName:RAXMLNG {
        cpus   = { check_max( 32 * task.attempt, 'cpus' ) }
        memory = { check_max( 128.GB * task.attempt, 'memory' ) }
        time   = { check_max( 48.h * task.attempt, 'time' ) }
        queue  = params.hpc_partition_normal
        clusterOptions = { 
            def options = '--mail-type=FAIL --exclusive'
            if (params.hpc_account) options += " --account=${params.hpc_account}"
            if (params.hpc_qos) options += " --qos=${params.hpc_qos}"
            return options
        }
    }
}

// Workdir cleanup (optional)
cleanup = false

// Reporting
report {
    enabled = true
    file    = "${params.outdir}/pipeline_info/execution_report.html"
}

timeline {
    enabled = true
    file    = "${params.outdir}/pipeline_info/execution_timeline.html"
}

trace {
    enabled = true
    file    = "${params.outdir}/pipeline_info/execution_trace.txt"
    fields  = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes,vol_ctxt,inv_ctxt'
}